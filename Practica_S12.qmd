---
title: "PRACTICA REGRESION LINEAL SIMPLE."
author: "Nelson de Jesus Magaña Godinez"
format: pdf
editor: visual
---

## 1. REGRESION LINEAL SIMPLE.

Los modelos de regresión lineal son modelos probabilísticos basados en una función lineal, expresamos el valor de nuestra variable de estudio (interés), a la que también llamamos variable dependiente, en función de una o más variables a quienes llamamos variables independientes o explicativas, y las cuales suponemos tienen un efecto sobre nuestra variable de estudio. Los pasos básicos a seguir en el estudio de un modelo lineal son:

-   Escribir el modelo matemático con todas sus hipótesis.

-   Estimación de los parámetros del modelo.

-   Inferencias sobre los parámetros.

-   Diagnóstico del modelo.

El modelo de regresión más simple que nos podemos encontrar es aquel en donde únicamente se considera a solamente una variable independiente, y se quiere estudiar su efecto sobre la variable dependiente; la ecuación del modelo es:

$$
Y_i=\beta_0+\beta_1x_i+u_i
$$

Donde:

-   $y_i$ ; representa la observación i-ésima correspondiente de la variable dependiente, es decir, el valor de la variable dependiente para el i-ésimo individuo de la muestra.

-   $x_i$ ; representa la observación i-ésima correspondiente de la variable independiente.

-   $\beta_0$ ; representa el intercepto del modelo, es decir, valor de la variable dependiente cuando nuestra variable independiente toma el valor de cero. En muchos casos no tendrá interpretación, pues la variable independiente no puede tomar el valor de 0.

-   $\beta_1$ ; representa la pendiente del modelo, es decir, el cambio esperado en la variable dependiente por cada cambio unitario realizado a la variable independiente.

-   $u_i$ ; representa el efecto de las demás variables omitidas en el modelo.

Las hipótesis básicas del modelo, son las mismas a las consideradas en el Análisis de Varianza, que como recordarán son las siguientes:

-   El promedio de las perturbaciones es cero, es decir, se cumple que:

    $E[u_i]=0; \forall_i$

-   La varianza de las perturbaciones es constante, es decir, se cumple que:

    $var(u_i)=\sigma^2; \forall_i$

-   La distribución de las perturbaciones debe ser normal, es decir se cumple que:

    $u_i\approx N(0;\sigma^2); \forall_i$

Las perturbaciones son independientes, es decir se cumple que:

$cov(u_i:u_j)=0; \forall_i$

Las cuales pueden resumirse en: $u_i\sim NIID(0;\sigma^2); \forall_i$

En R la función a utilizar para realizar o ajustar un modelo de regresión es lm() (de lineal model). Esta función no nos ofrece ninguna salida en pantalla si no que nos crea un objeto, o mejor dicho, nosotros creamos un objeto que va a ser un modelo de regresión lineal, y el cual podemos referenciarlo posteriormente en nuestro análisis.

La función lm tiene la siguiente sintaxis: $lm(formula, data, subset)$

-   En formula escribimos: $y \sim x$ , lo cual significa que a la izquierda del símbolo $\sim$ especificamos quien es nuestra variable dependiente; mientras que a la derecha especificamos quien es nuestra variable independiente.

-   En data especificamos el dataframe que contiene las variables del modelo, es recomendable que los datos se encuentren en un dataframe.

-   En subset especificamos un subconjunto de observaciones para validar posteriormente el modelo. En caso que se desee utilizar conjuntos distinto para estimar y validar el modelo. Muy recomendado en muchas aplicaciones.

La función lm tiene muchas más opciones pero para conocer mejor su funcionamiento vamos a ver

ejemplos.

-   EJEMPLO 1.

En el archivo "costes.dat" se encuentra la información correspondiente a 34 fábricas de producción en el montaje de placas para ordenador, el archivo contiene la información sobre el costo total (primera columna) y el número de unidades fabricadas (segunda columna). Suponga que deseamos ajustar un modelo de regresión simple a los datos para estimar el costo total en función del número de unidades

fabricadas.

Ejecutamos lo siguiente.

Lectura de los datos

```{r}
datos=read.table("costes.dat.txt")
dim(datos)
```

La base de datos contiene 35 filas (observaciones) en dos columnas (2 variables)

Renombrando las variables

```{r}
names(datos)=c("Costos", "Unidades")
names(datos)
```

Diagrama de dispersion entre las dos variables

```{r}
plot(datos$Unidades, datos$Costos)
```

Se aprecia una relación creciente entre las variables por lo que se procede a ajustar el modelo de regresión.

```{r}
#regresion = lm(datos$Costos ~ datos$Unidades)
summary(regresion)
```

El modelo de regresion resultante seria

Datos=33.92200+0.09640(Unidades)

Se observa que el término constante no es significativo porque el p-valor correspondiente a la prueba

de hipótesis $H_0 :\beta_0 = 0$ es $0.501$; y además no tiene interpretación, pues en teoría si no se fabrican unidades no deberían existir costos asociados a la producción.

Como el término constante no es significativo se quitara del modelo, volvemos a realizar los cálculos con el R.

**Ejecutar lo siguiente.**

```{r}
regresion2 <- lm(datos$Costos~datos$Unidades-1)
summary(regresion2)
```

En este caso el modelo resultante sería: costos = 0.13550 (unidades); el cual es un mejor modelo en términos de variabilidad explicada.

Una vez estimados los parámetros del modelo, el siguiente paso es validarlo, es decir verificar si se cumplen las cuatro hipótesis básicas del modelo (nulidad, normalidad, independencia y homocesdasticidad de los residuos). Para verificar esto, podríamos realizar los siguientes pasos:

```{r}
#Efectúa un análisis gráfico de bondad de ajuste del modelo
par(mfrow = c(2, 2))
plot(regresion2)
par(oma = c(1,1,1,1), new = T, font = 2, cex = 0.5)
mtext(outer = T, "Gráficos para validación del modelo: Costos en función de las unidades",
side = 3)
```

En los gráficos que se muestra en la parte superior se contrasta los cuatro supuestos. En el de la izquierda se verifican: nulidad, independencia y homocedasticidad; a partir del gráfico mostrado parece existir indicios de falta de homocedasticidad, por su parte los residuos pueden considerarse constante pues no muestran ningún patrón; sin embargo, la media de los residuos no parece ser nula, lo cual indica falta de linealidad en el modelo (es decir, es necesario incorporar más variables o tal vez términos cuadráticos). En la figura de la derecha se contrasta la normalidad, y puede apreciarse que los residuos parecen seguir una distribución normal.

Por su parte, también es de mencionar que en el gráfico se muestran puntos que posiblemente sean observaciones atípicas, por lo que habría que estudiarlas.

```{r}
# Información sobre el modelo ajustado que proporciona la función lm()
formula(regresion2) # Extrae la fórmula del modelo
```

```{r}
coef(regresion2) # Extrae el vector de coeficientes de regresión.
```

```{r}
model.offset(regresion2)
#modelo2ted.values(regresion2) # Extrae un vector con los valores estimados.
```

```{r}
vcov(regresion2) # Extrae la matriz de covarianzas de los parámetros.
```

```{r}
ls.diag(regresion2) # Calcula los residuales, errores estándar de los parámetros, distancias Cook.
```

```{r}
step(regresion2) # Permite obtener el mejor conjunto de regresión y proporciona la estimación de los coeficientes (válido únicamente en modelos de regresión múltiple).
```

de todos los resultados anteriores nos concentraremos en la instrucción: ls.diag(regresion2). Con esta instrucción obtenemos para cada observación en el conjunto de datos, medidas que nos ayudarán a identificar observación atípicas (tienen un impacto únicamente en las medidas resumen del modelo) y observaciones influyentes (tienen un efecto marcado en la estimación de los parámetros). Al digitar la instrucción anterior en R se mostrará los siguientes resultados (cada uno de ellos en un vector).

-   \$hat. Corresponde a los elementos de la diagonal de la matriz $H=X(X'X)^{-1}X'$, y se examina $H_{ii}$ que mide la distancia de $H_{i}$ (observación i-ésima) al centro de los datos (medida estandarizada). Los elementos grandes indican observaciones potencialmente influyentes. Si se cumple que $H_{ii}>2(\frac{k+1}{n})$ se trata de una observación influyente.

-   \$std.res. Son los residuos estandarizados (la varianza de los residuos se supone es la misma) del modelo. Una observación se considera influyente si su residuo estandarizado es mayor en valor absoluto a 3.

-   \$stud.res. Son los residuos estudentizados del modelo (se considera que la varianza de los residuos es diferente); estos residuos siguen una distribución t de Student para $n-3$ grados de libertad. Por lo que si para una observación su residuo estandarizado es mayor en valor absoluto al percentil 95 de la distribución t de Student se considera como punto influyente.

-   \$cooks. Es la distancia de Cook (mide el efecto de eliminar una observación en la estimación de cada de los parámetros, el efecto se mide en desviaciones típicas). Si dicha distancia es mayor a 1 el punto se considera como influyente.

-   \$dfits. Es el valor del DDFFITS (mide el cambio ocurrido en la estimación de una observación cuando esta observación es descartada y luego incluida en el modelo). Se considera que una observación es influyente si su correspondiente DDFFITS es mayor, en valor absoluto, a $2\sqrt{\frac{k+1}{n}}$ Donde k es el número de variables en el modelo (en regresión simple es igual a 1).

## 2. REGRESIÓN LINEAL MÚLTIPLE

Al igual que en el modelo de regresión simple, el modelo de regresión múltiple trata de ajustar una ecuación matemática en la que se relacione a una única variable dependiente en función de dos o más variables independientes. La forma general del modelo es la siguiente:

$$
y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+...+\beta_kx_{ki}+u_i
$$

Como siempre debe cumplirse que: $u_i\sim NID(0, \sigma^2);\forall_i$

La función para estimar cada uno de los parámetros del modelo, a partir de la información suministrada por la muestra, los datos disponibles, es como siempre lm(), sin embargo, en la expresión fórmula debemos escribir $y\sim x_1+x_2+x_3+...+x_k$ Todas las instrucciones utilizadas en regresión simple son válidas también para regresión múltiple (diagnosis de los residuos e identificación de puntos influyentes).

Veamos el siguiente ejemplo.

-   **Ejemplo 2.**

    En el archivo "preciocasas.dat" tienen la información sobre 100 datos de precios de viviendas y sus características, el archivo se encuentra estructurado de la siguiente forma:

-   Primera columna: precios de viviendas en euros.

-   Segunda columna: superficie en metros cuadrados.

-   Tercera: numero de cuartos de baño.

-   Cuarta: número de dormitorios.

-   Quinta: número de plazas de garaje.

-   Sexta: edad de la vivienda .

-   Séptima: 1 =buenas vistas y 0 =vistas corrientes

Suponga que deseamos estimar un modelo de regresión en el cual relacionemos el precio de una vivienda en función de sus características.

**Ejecutar lo siguiente:**

```{r}
datos <- read.table(file = "preciocasas.dat.txt")
```

```{r}
# nombrando a las columnas
names(datos) <- c("precio", "x1", "x2", "x3", "x4", "x5", "x6")
```

```{r}
# haciendo la matriz de diagramas de dispersión
plot(datos)
```

Se observa gráficamente que las variables independientes parecen influir en el comportamiento de nuestra variable dependiente.

```{r}
# ajustamos el modelo de regresión
modelo1 <- lm(precio ~ x1 + x2 + x3 + x4 + x5 + x6 , data = datos)
```

```{r}
#resumen del modelo
summary(modelo1)
```

De los resultados anteriores puede apreciarse que el intercepto, y las variables x2 (número de cuarto de baño) y x3 (número de dormitorios) no parecen influir en la estimación del precio de la vivienda por lo podrían descartarse de la ecuación.

Una forma alternativa y mucho más eficiente para seleccionar el mejor conjunto de variables independientes es utilizar la instrucción step(), con la cual se utilizan los algoritmos conocidos para seleccionar variables (selección hacia adelante -"forward"-, hacia atrás -"backward"- o selección por pasos -"both"-).

```{r}
step(modelo1, direction="both")
```

-   **EJERCICIO 1.**

Se deja como ejercicio al estudiante, elegir el mejor conjunto de variables a incluir en el modelo, y para el modelo resultante (llamarlo modelo2), realizar el diagnóstico de los residuos y el estudio de las observaciones atípicas e influyentes.

```{r}
modelo2<-lm(precio ~x2 + x4 +x5, data = datos)
coefficients(modelo2) # coeficientes del modelo
```

```{r}
confint(modelo2, level=0.95) # Intevalos de confianza para los parámetros
```

```{r}
fitted(modelo2) # valores estimados
```

```{r}
residuals(modelo2) # residuos
```

```{r}
influence(modelo2) # puntos de influencia
```
